{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CapCheck AI Detection - ViT Fine-Tuning\n",
    "\n",
    "Fine-tune the `dima806/ai_vs_real_image_detection` model on newer AI-generated images.\n",
    "\n",
    "**Goals:**\n",
    "- Train on Flux, Midjourney v6, DALL-E 3, SD3 generated images\n",
    "- Reduce false positives (real flagged as AI)\n",
    "- Reduce false negatives (AI slipping through)\n",
    "\n",
    "**Environment:** Google Colab (T4 GPU) or Local (MPS/CUDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run in Colab)\n",
    "# !pip install -q transformers datasets evaluate accelerate huggingface_hub pillow scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import evaluate\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Check available device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(\"Using Apple MPS\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Using CPU (this will be slow)\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "\n",
    "# Base model to fine-tune\n",
    "BASE_MODEL = \"dima806/ai_vs_real_image_detection\"\n",
    "\n",
    "# Your HuggingFace model name (where we'll push the fine-tuned model)\n",
    "HF_MODEL_NAME = \"capcheck/ai-image-detection\"\n",
    "\n",
    "# Dataset paths - adjust based on where your data is\n",
    "# For Colab: mount Google Drive and point to your dataset\n",
    "# For local: use relative or absolute path\n",
    "DATA_DIR = \"./data\"  # Change this!\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 16  # Reduce to 8 if you get OOM errors\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 3\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"./checkpoints\"\n",
    "\n",
    "# Labels\n",
    "LABELS = [\"real\", \"ai\"]  # 0 = real, 1 = ai\n",
    "ID2LABEL = {0: \"Real\", 1: \"Fake\"}\n",
    "LABEL2ID = {\"Real\": 0, \"Fake\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mount Google Drive (Colab Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if using Google Colab and your data is in Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# DATA_DIR = \"/content/drive/MyDrive/capcheck-training-data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading base model: {BASE_MODEL}\")\n",
    "\n",
    "# Load image processor (handles resizing, normalization)\n",
    "image_processor = AutoImageProcessor.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    id2label=ID2LABEL,\n",
    "    label2id=LABEL2ID,\n",
    "    ignore_mismatched_sizes=True,  # In case labels differ\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {model.config.num_labels} classes\")\n",
    "print(f\"Parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Dataset\n",
    "\n",
    "Expected structure:\n",
    "```\n",
    "data/\n",
    "├── train/\n",
    "│   ├── ai/      # AI-generated images\n",
    "│   └── real/    # Real photographs\n",
    "├── val/\n",
    "│   ├── ai/\n",
    "│   └── real/\n",
    "└── test/\n",
    "    ├── ai/\n",
    "    └── real/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_folder_dataset(data_dir):\n",
    "    \"\"\"Load dataset from ImageFolder structure.\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    \n",
    "    datasets = {}\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        split_dir = data_dir / split\n",
    "        if not split_dir.exists():\n",
    "            print(f\"Warning: {split_dir} not found, skipping\")\n",
    "            continue\n",
    "            \n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        # Load real images (label = 0)\n",
    "        real_dir = split_dir / \"real\"\n",
    "        if real_dir.exists():\n",
    "            for img_path in real_dir.glob(\"*\"):\n",
    "                if img_path.suffix.lower() in [\".jpg\", \".jpeg\", \".png\", \".webp\"]:\n",
    "                    images.append(str(img_path))\n",
    "                    labels.append(0)  # Real = 0\n",
    "        \n",
    "        # Load AI images (label = 1)\n",
    "        ai_dir = split_dir / \"ai\"\n",
    "        if ai_dir.exists():\n",
    "            for img_path in ai_dir.glob(\"*\"):\n",
    "                if img_path.suffix.lower() in [\".jpg\", \".jpeg\", \".png\", \".webp\"]:\n",
    "                    images.append(str(img_path))\n",
    "                    labels.append(1)  # AI = 1\n",
    "            # Also check subdirectories (flux/, midjourney/, etc.)\n",
    "            for subdir in ai_dir.iterdir():\n",
    "                if subdir.is_dir():\n",
    "                    for img_path in subdir.glob(\"*\"):\n",
    "                        if img_path.suffix.lower() in [\".jpg\", \".jpeg\", \".png\", \".webp\"]:\n",
    "                            images.append(str(img_path))\n",
    "                            labels.append(1)\n",
    "        \n",
    "        if images:\n",
    "            datasets[split] = Dataset.from_dict({\n",
    "                \"image_path\": images,\n",
    "                \"label\": labels,\n",
    "            })\n",
    "            print(f\"{split}: {len(images)} images (Real: {labels.count(0)}, AI: {labels.count(1)})\")\n",
    "    \n",
    "    return DatasetDict(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(f\"Loading dataset from: {DATA_DIR}\")\n",
    "dataset = load_image_folder_dataset(DATA_DIR)\n",
    "print(f\"\\nLoaded splits: {list(dataset.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"Load and preprocess images for the model.\"\"\"\n",
    "    images = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for idx, path in enumerate(examples[\"image_path\"]):\n",
    "        try:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            valid_indices.append(idx)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Process images\n",
    "    inputs = image_processor(images, return_tensors=\"pt\")\n",
    "    \n",
    "    # Get labels for valid images only\n",
    "    labels = [examples[\"label\"][i] for i in valid_indices]\n",
    "    \n",
    "    return {\n",
    "        \"pixel_values\": inputs[\"pixel_values\"],\n",
    "        \"label\": labels,\n",
    "    }\n",
    "\n",
    "\n",
    "def transform_for_training(example):\n",
    "    \"\"\"Transform a single example - used with set_transform for lazy loading.\"\"\"\n",
    "    try:\n",
    "        img = Image.open(example[\"image_path\"]).convert(\"RGB\")\n",
    "        inputs = image_processor(img, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"pixel_values\": inputs[\"pixel_values\"].squeeze(0),\n",
    "            \"label\": example[\"label\"],\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transforms (lazy loading - more memory efficient)\n",
    "for split in dataset:\n",
    "    dataset[split].set_transform(transform_for_training)\n",
    "\n",
    "print(\"Transforms applied. Images will be loaded on-demand.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute metrics for evaluation.\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "        \"precision\": precision_metric.compute(predictions=predictions, references=labels, average=\"binary\")[\"precision\"],\n",
    "        \"recall\": recall_metric.compute(predictions=predictions, references=labels, average=\"binary\")[\"recall\"],\n",
    "        \"f1\": f1_metric.compute(predictions=predictions, references=labels, average=\"binary\")[\"f1\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # Training params\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    \n",
    "    # Optimization\n",
    "    fp16=device == \"cuda\",  # Mixed precision on CUDA\n",
    "    gradient_accumulation_steps=2,\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",  # Set to \"wandb\" for W&B tracking\n",
    "    \n",
    "    # Misc\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,  # We'll push manually after training\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom data collator to handle our format\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate batch of examples.\"\"\"\n",
    "    # Filter out None values (failed image loads)\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    \n",
    "    pixel_values = torch.stack([b[\"pixel_values\"] for b in batch])\n",
    "    labels = torch.tensor([b[\"label\"] for b in batch])\n",
    "    \n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset.get(\"train\"),\n",
    "    eval_dataset=dataset.get(\"val\"),\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Training complete!\")\n",
    "print(f\"Best model saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "if \"test\" in dataset:\n",
    "    print(\"Evaluating on test set...\")\n",
    "    test_results = trainer.evaluate(dataset[\"test\"])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"TEST RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "    for key, value in test_results.items():\n",
    "        if key.startswith(\"eval_\"):\n",
    "            print(f\"{key.replace('eval_', ''):15}: {value:.4f}\")\n",
    "else:\n",
    "    print(\"No test set found. Showing validation results.\")\n",
    "    val_results = trainer.evaluate()\n",
    "    for key, value in val_results.items():\n",
    "        if key.startswith(\"eval_\"):\n",
    "            print(f\"{key.replace('eval_', ''):15}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "def plot_confusion_matrix(trainer, dataset_split):\n",
    "    \"\"\"Generate and plot confusion matrix.\"\"\"\n",
    "    predictions = trainer.predict(dataset_split)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    labels = predictions.label_ids\n",
    "    \n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt=\"d\", \n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=[\"Real\", \"AI\"],\n",
    "        yticklabels=[\"Real\", \"AI\"],\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(labels, preds, target_names=[\"Real\", \"AI\"]))\n",
    "\n",
    "# Plot for test or validation set\n",
    "eval_split = \"test\" if \"test\" in dataset else \"val\"\n",
    "if eval_split in dataset:\n",
    "    plot_confusion_matrix(trainer, dataset[eval_split])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Model Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "SAVE_PATH = f\"{OUTPUT_DIR}/capcheck-ai-detection-final\"\n",
    "\n",
    "trainer.save_model(SAVE_PATH)\n",
    "image_processor.save_pretrained(SAVE_PATH)\n",
    "\n",
    "print(f\"Model saved to: {SAVE_PATH}\")\n",
    "print(f\"Contents: {os.listdir(SAVE_PATH)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Push to HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "# Option 1: Use environment variable\n",
    "# os.environ[\"HF_TOKEN\"] = \"your-token-here\"\n",
    "\n",
    "# Option 2: Interactive login\n",
    "login()  # This will prompt for your token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push model to HuggingFace Hub\n",
    "print(f\"Pushing model to: {HF_MODEL_NAME}\")\n",
    "\n",
    "# Push using the trainer\n",
    "trainer.push_to_hub(\n",
    "    repo_id=HF_MODEL_NAME,\n",
    "    commit_message=\"Fine-tuned ViT for AI image detection\",\n",
    ")\n",
    "\n",
    "# Also push the image processor\n",
    "image_processor.push_to_hub(HF_MODEL_NAME)\n",
    "\n",
    "print(f\"\\nModel published to: https://huggingface.co/{HF_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and push model card\n",
    "MODEL_CARD = f\"\"\"\n",
    "---\n",
    "license: apache-2.0\n",
    "base_model: {BASE_MODEL}\n",
    "tags:\n",
    "- image-classification\n",
    "- vision\n",
    "- ai-detection\n",
    "- deepfake-detection\n",
    "datasets:\n",
    "- custom\n",
    "metrics:\n",
    "- accuracy\n",
    "- f1\n",
    "- precision\n",
    "- recall\n",
    "---\n",
    "\n",
    "# CapCheck AI Image Detection\n",
    "\n",
    "Fine-tuned Vision Transformer (ViT) for detecting AI-generated images.\n",
    "\n",
    "## Model Description\n",
    "\n",
    "This model is fine-tuned from `{BASE_MODEL}` on a custom dataset of modern AI-generated images including:\n",
    "- Flux\n",
    "- Midjourney v6\n",
    "- DALL-E 3\n",
    "- Stable Diffusion 3\n",
    "\n",
    "## Training Details\n",
    "\n",
    "- **Base Model**: {BASE_MODEL}\n",
    "- **Epochs**: {NUM_EPOCHS}\n",
    "- **Batch Size**: {BATCH_SIZE}\n",
    "- **Learning Rate**: {LEARNING_RATE}\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "detector = pipeline(\"image-classification\", model=\"{HF_MODEL_NAME}\")\n",
    "result = detector(\"path/to/image.jpg\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "## Labels\n",
    "\n",
    "- `Real`: Authentic photograph\n",
    "- `Fake`: AI-generated image\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- Performance may vary on image types not seen during training\n",
    "- Heavily compressed images may reduce accuracy\n",
    "- New AI generators not in training data may evade detection\n",
    "\n",
    "## License\n",
    "\n",
    "Apache 2.0\n",
    "\"\"\"\n",
    "\n",
    "# Save model card\n",
    "with open(f\"{SAVE_PATH}/README.md\", \"w\") as f:\n",
    "    f.write(MODEL_CARD)\n",
    "\n",
    "# Push model card to hub\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=f\"{SAVE_PATH}/README.md\",\n",
    "    path_in_repo=\"README.md\",\n",
    "    repo_id=HF_MODEL_NAME,\n",
    "    commit_message=\"Add model card\",\n",
    ")\n",
    "\n",
    "print(\"Model card uploaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Test the Published Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model from HuggingFace Hub\n",
    "from transformers import pipeline\n",
    "\n",
    "print(f\"Testing model from: {HF_MODEL_NAME}\")\n",
    "\n",
    "detector = pipeline(\"image-classification\", model=HF_MODEL_NAME)\n",
    "\n",
    "# Test with a sample image\n",
    "# test_image = \"path/to/test/image.jpg\"\n",
    "# result = detector(test_image)\n",
    "# print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After publishing to HuggingFace:\n",
    "\n",
    "1. Update `ml/services/ai-image-detection/predict.py`:\n",
    "   ```python\n",
    "   MODEL_REGISTRY = {\n",
    "       \"v1.0.0\": \"dima806/ai_vs_real_image_detection\",\n",
    "       \"v1.1.0\": \"capcheck/ai-image-detection\",  # New!\n",
    "   }\n",
    "   ```\n",
    "\n",
    "2. Update `ml/services/ai-image-detection/cog.yaml` to pre-download new model\n",
    "\n",
    "3. Push to Replicate:\n",
    "   ```bash\n",
    "   cd ml/services/ai-image-detection\n",
    "   cog push r8.im/your-username/ai-image-detection\n",
    "   ```\n",
    "\n",
    "4. Update backend with new Replicate version hash"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
